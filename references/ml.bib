@ARTICLE{Bond-Taylor2021-oj,
  title         = "Deep Generative Modelling: A Comparative Review of {VAEs},
                   {GANs}, Normalizing Flows, {Energy-Based} and Autoregressive
                   Models",
  author        = "Bond-Taylor, Sam and Leach, Adam and Long, Yang and
                   Willcocks, Chris G",
  abstract      = "Deep generative modelling is a class of techniques that
                   train deep neural networks to model the distribution of
                   training samples. Research has fragmented into various
                   interconnected approaches, each of which making trade-offs
                   including run-time, diversity, and architectural
                   restrictions. In particular, this compendium covers
                   energy-based models, variational autoencoders, generative
                   adversarial networks, autoregressive models, normalizing
                   flows, in addition to numerous hybrid approaches. These
                   techniques are drawn under a single cohesive framework,
                   comparing and contrasting to explain the premises behind
                   each, while reviewing current state-of-the-art advances and
                   implementations.",
  month         =  mar,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.04922",
  archivePrefix = "arXiv",
  eprint        = "2103.04922",
  primaryClass  = "cs.LG",
  arxivid       = "2103.04922"
}

@ARTICLE{Lamb2021-uq,
  title         = "A Brief Introduction to Generative Models",
  author        = "Lamb, Alex",
  abstract      = "We introduce and motivate generative modeling as a central
                   task for machine learning and provide a critical view of the
                   algorithms which have been proposed for solving this task.
                   We overview how generative modeling can be defined
                   mathematically as trying to make an estimating distribution
                   the same as an unknown ground truth distribution. This can
                   then be quantified in terms of the value of a statistical
                   divergence between the two distributions. We outline the
                   maximum likelihood approach and how it can be interpreted as
                   minimizing KL-divergence. We explore a number of approaches
                   in the maximum likelihood family, while discussing their
                   limitations. Finally, we explore the alternative adversarial
                   approach which involves studying the differences between an
                   estimating distribution and a real data distribution. We
                   discuss how this approach can give rise to new divergences
                   and methods that are necessary to make adversarial learning
                   successful. We also discuss new evaluation metrics which are
                   required by the adversarial approach.",
  month         =  feb,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.00265",
  archivePrefix = "arXiv",
  eprint        = "2103.00265",
  primaryClass  = "cs.LG",
  arxivid       = "2103.00265"
}

@ARTICLE{Agliari2021-ix,
  title         = "The relativistic Hopfield model with correlated patterns",
  author        = "Agliari, Elena and Fachechi, Alberto and Marullo, Chiara",
  abstract      = "In this work we introduce and investigate the properties of
                   the ``relativistic'' Hopfield model endowed with temporally
                   correlated patterns. First, we review the ``relativistic''
                   Hopfield model and we briefly describe the experimental
                   evidence underlying correlation among patterns. Then, we
                   face the study of the resulting model exploiting
                   statistical-mechanics tools in a low-load regime. More
                   precisely, we prove the existence of the thermodynamic limit
                   of the related free-energy and we derive the
                   self-consistence equations for its order parameters. These
                   equations are solved numerically to get a phase diagram
                   describing the performance of the system as an associative
                   memory as a function of its intrinsic parameters (i.e., the
                   degree of noise and of correlation among patterns). We find
                   that, beyond the standard retrieval and ergodic phases, the
                   relativistic system exhibits correlated and symmetric
                   regions -- that are genuine effects of temporal correlation
                   -- whose width is, respectively, reduced and increased with
                   respect to the classical case.",
  month         =  mar,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.06259",
  archivePrefix = "arXiv",
  eprint        = "2103.06259",
  primaryClass  = "math-ph",
  arxivid       = "2103.06259"
}

@ARTICLE{Finn2016-yc,
  title         = "A Connection between Generative Adversarial Networks,
                   Inverse Reinforcement Learning, and {Energy-Based} Models",
  author        = "Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and
                   Levine, Sergey",
  abstract      = "Generative adversarial networks (GANs) are a recently
                   proposed class of generative models in which a generator is
                   trained to optimize a cost function that is being
                   simultaneously learned by a discriminator. While the idea of
                   learning cost functions is relatively new to the field of
                   generative modeling, learning costs has long been studied in
                   control and reinforcement learning (RL) domains, typically
                   for imitation learning from demonstrations. In these fields,
                   learning cost function underlying observed behavior is known
                   as inverse reinforcement learning (IRL) or inverse optimal
                   control. While at first the connection between cost learning
                   in RL and cost learning in generative modeling may appear to
                   be a superficial one, we show in this paper that certain IRL
                   methods are in fact mathematically equivalent to GANs. In
                   particular, we demonstrate an equivalence between a
                   sample-based algorithm for maximum entropy IRL and a GAN in
                   which the generator's density can be evaluated and is
                   provided as an additional input to the discriminator.
                   Interestingly, maximum entropy IRL is a special case of an
                   energy-based model. We discuss the interpretation of GANs as
                   an algorithm for training energy-based models, and relate
                   this interpretation to other recent work that seeks to
                   connect GANs and EBMs. By formally highlighting the
                   connection between GANs, IRL, and EBMs, we hope that
                   researchers in all three communities can better identify and
                   apply transferable ideas from one domain to another,
                   particularly for developing more stable and scalable
                   algorithms: a major challenge in all three domains.",
  month         =  nov,
  year          =  2016,
  url           = "http://arxiv.org/abs/1611.03852",
  archivePrefix = "arXiv",
  eprint        = "1611.03852",
  primaryClass  = "cs.LG",
  arxivid       = "1611.03852"
}
