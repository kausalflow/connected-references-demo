@ARTICLE{Yang2019-xt,
  title     = "A survey on canonical correlation analysis",
  author    = "Yang, Xinghao and Weifeng, Liu and Liu, Wei and Tao, Dacheng",
  journal   = "IEEE Trans. Knowl. Data Eng.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  pages     = "1--1",
  year      =  2019,
  url       = "https://ieeexplore.ieee.org/document/8928538/",
  issn      = "1041-4347, 1558-2191",
  doi       = "10.1109/tkde.2019.2958342"
}

@BOOK{Shalev-Shwartz2013-ac,
  title    = "Understanding machine learning: From theory to algorithms",
  author   = "Shalev-Shwartz, Shai and Ben-David, Shai",
  abstract = "Machine learning is one of the fastest growing areas of computer
              science, with far-reaching applications. The aim of this textbook
              is to introduce machine learning, and the algorithmic paradigms
              it offers, in a principled way. The book provides an extensive
              theoretical account of the fundamental ideas underlying machine
              learning and the mathematical derivations that transform these
              principles into practical algorithms. Following a presentation of
              the basics of the field, the book covers a wide array of central
              topics that have not been addressed by previous textbooks. These
              include a discussion of the computational complexity of learning
              and the concepts of convexity and stability; important
              algorithmic paradigms including stochastic gradient descent,
              neural networks, and structured output learning; and emerging
              theoretical concepts such as the PAC-Bayes approach and
              compression-based bounds. Designed for an advanced undergraduate
              or beginning graduate course, the text makes the fundamentals and
              algorithms of machine learning accessible to students and
              non-expert readers in statistics, computer science, mathematics,
              and engineering.",
  volume   =  9781107057,
  pages    = "1--397",
  year     =  2013,
  url      = "http://dx.doi.org/10.1017/CBO9781107298019",
  isbn     = "9781107298019",
  doi      = "10.1017/CBO9781107298019"
}

@INPROCEEDINGS{Bender2021-fi,
  title      = "On the dangers of stochastic parrots",
  booktitle  = "Proceedings of the 2021 {ACM} Conference on Fairness,
                Accountability, and Transparency",
  author     = "Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina
                and Shmitchell, Shmargaret",
  publisher  = "ACM",
  month      =  mar,
  year       =  2021,
  url        = "http://dx.doi.org/10.1145/3442188.3445922",
  address    = "New York, NY, USA",
  conference = "FAccT '21: 2021 ACM Conference on Fairness, Accountability, and
                Transparency",
  location   = "Virtual Event Canada",
  isbn       = "9781450383097",
  doi        = "10.1145/3442188.3445922"
}

@BOOK{noauthor_1998-zp,
  title = "Data Science Classification And Related Methods",
  year  =  1998
}

@TECHREPORT{Novikoff1963-at,
  title       = "On convergence proofs for perceptrons",
  author      = "Novikoff, Albert B",
  institution = "STANFORD RESEARCH INST MENLO PARK CA",
  year        =  1963,
  url         = "http://classes.engr.oregonstate.edu/eecs/fall2017/cs534/extra/novikoff-1963.pdf"
}

@ARTICLE{Domingos2012-wn,
  title    = "A few useful things to know about machine learning",
  author   = "Domingos, Pedro",
  abstract = "MACHINE LEARNING SYSTEMS automatically learn programs from data.
              This is often a very attractive alternative to manually
              constructing them, and in the last decade the use of machine
              learning has spread rapidly throughout computer science and
              beyond. Machine learning is used in Web search, spam filters,
              recommender systems, ad placement, credit scoring, fraud
              detection, stock trading, drug design, and many other
              applications. A recent report from the McKinsey Global Institute
              asserts that machine learning (a.k.a. data mining or predictive
              analytics) will be the driver of the next big wave of innovation.
              15 Several fine textbooks are available to interested
              practitioners and researchers (for example, Mitchell 16 and
              Witten et al. 24). However, much of the ``folk knowledge'' that
              is needed to successfully develop machine learning applications
              is not readily available in them. As a result, many machine
              learning projects take much longer than necessary or wind up
              producing less-than-ideal results. Yet much of this folk
              knowledge is fairly easy to communicate. This is the purpose of
              this article. \copyright{} 2012 ACM.",
  journal  = "Commun. ACM",
  volume   =  55,
  number   =  10,
  pages    = "78--87",
  month    =  oct,
  year     =  2012,
  url      = "https://dl.acm.org/doi/10.1145/2347736.2347755",
  issn     = "0001-0782",
  doi      = "10.1145/2347736.2347755"
}

@ARTICLE{Melko_undated-gy,
  title    = "Restricted Boltzmann machines in quantum physics",
  author   = "Melko, Roger G and Carleo, Giuseppe and Carrasquilla, Juan and
              Cirac, J Ignacio",
  abstract = "T he study of strongly correlated quantum many-body physics is
              the last frontier still untamed by the sophisticated,
              high-performance computational frameworks pervasive in other
              areas of the sciences. The challenge is fundamental; how does one
              use classical computers to study the many-body quantum
              wavefunction, an object inherently fraught with exponentially
              scaling complexity? Researchers have developed a variety of tools
              over the years to attempt to tame this complexity, such as the
              quantum Monte Carlo and density matrix renormalization group
              (DMRG) methods 1-3. Despite this, key problems remain unsolved in
              condensed-matter physics, ranging from a numerical solution to
              the Hubbard model to the origins of thermalization from quantum
              dynamics, and more. These problems share deep connections with
              challenges in the quantum information sciences, where
              experimentalists are now obtaining sufficient control of
              individual qubits and their interactions to enable the
              engineering of coherent many-body devices 4,5. There,
              computational tools will be key in the reconstruction and
              characterization of wavefunctions from experimental measurements
              6 , a necessary step in the design, construction and validation
              of quantum computers. Enter machine learning with its recent
              high-profile success in a range of real-world problems, such as
              computer vision and natural language processing, each exhibiting
              a complexity that scales surprisingly similar to the quantum
              many-body problem 7,8. The quantum physics community has taken
              notice, throwing themselves headfirst into an exploration of the
              algorithms underlying modern machine learning, with an eye on
              making progress in quantum physics. Among the various approaches
              involving supervised, unsupervised and reinforcement learning,
              restricted Boltzmann machines (RBMs) stand out as objects
              superbly interpretable under the edifice of well-established
              statistical physics and quantum information theories. We begin
              this Perspective with a review of the representational power of
              RBMs, and their relationship to that most elusive object: the
              quantum wavefunc-tion. Guided by the extensive frameworks already
              in existence for the simulation of quantum systems on classical
              computers, we discuss two different paradigms for the 'learning'
              of the RBM parameters, varia-tional minimization of the energy 7
              and reconstruction from experimental data 9,10. We close with a
              perspective on the future of this young field of quantum-state
              reconstruction with machine learning and its role in the coming
              age of 'quantum supremacy' 11. RBMs and efficient representation
              of quantum states Boltzmann machines were created as a way of
              graphically representing a probability distribution p(v), by
              defining a number of stochastic variables (nodes) v and an
              'energy' E by which the nodes interact (Box 1). Hopfield
              networks, familiar to physicists as fully connected Ising models,
              were the first to employ this paradigm to model asso-ciative
              memory 12. Hopfield's work established a connection between the
              concept of learning and the optimization of neural-network energy
              parameters (weights and biases). This in turn led to fruitful
              cross-pollination between machine learning and core concepts in
              statistical physics, such as glassiness and inverse Ising
              problems 13. Whereas Hopfield networks have a number of nodes
              equal to the size of the physical system under consideration,
              Boltzmann machines extend this space to include unphysical nodes
              and interactions 14. This hidden or latent space h can capture
              higher-order correlations between visible nodes, significantly
              increasing the expressiveness of the network representing the
              joint distribution p(v,h). To facilitate a practical algorithm
              for learning the network parameters from data, Boltzmann machines
              became restricted; that is, couplings were allowed only between
              visible and hidden units, but not intralayer (visible-visible or
              hidden-hidden) 15. Thus coined, RBMs played a historically
              important role in generative pre-training, dimensionality
              reduction, collaborative filtering, topic modelling and more
              16-18. Even today, RBMs offer a gold standard in explicit density
              models, where a well-understood Monte Carlo strategy is used to
              estimate the cost function. These features are particularly
              appealing to physicists, and constitute one of the reasons for
              the present rise of popularity of RBMs in the field of
              wavefunc-tion representation. An RBM is known to be a universal
              approximator 19. That is, given a sufficient number of hidden
              units, the network is capable of approximately representing any
              desired probability distribution to arbitrary accuracy. This
              capability immediately raises the prospect of using RBMs as
              approximate representations of many-body wave-functions, dubbed
              neural-network quantum states 7. However to do so, RBMs must
              first be modified to accommodate complex phases. This may be
              achieved in a number of ways, for example, by allowing the
              weights to be complex numbers 7 or by indirectly parameterizing a
              phase with additional hidden units 9. This conceptual leap, from
              probability distributions to complex-valued quantum amplitudes,
              sets a distinct shift from classical applications to quantum
              ones, and often demands the development of novel machine learning
              tools specific for quantum applications. Given that sufficiently
              large RBMs are universal approximators, it is natural to ask a
              theoretical question: precisely which types of quantum
              wavefunctions can they represent efficiently, that is, using",
  url      = "http://dx.doi.org/10.1038/s41567-019-0545-1",
  doi      = "10.1038/s41567-019-0545-1"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Waldchen2021-kf,
  title    = "The Computational Complexity of Understanding Binary Classifier
              Decisions",
  author   = "W{\"a}ldchen, Stephan and Macdonald, Jan and Hauch, Sascha and
              Kutyniok, Gitta",
  abstract = "For a d-ary Boolean function $\Phi$ : \{0, 1\} d $\rightarrow$
              \{0, 1\} and an assignment to its variables x = (x 1 , x 2 ,. ..
              , x d) we consider the problem of finding those subsets of the
              variables that are sufficient to determine the function value
              with a given probability $\delta$. This is motivated by the task
              of interpreting predictions of binary classifiers described as
              Boolean circuits, which can be seen as special cases of neural
              networks. We show that the problem of deciding whether such
              subsets of relevant variables of limited size k $\leq$ d exist is
              complete for the complexity class NP PP and thus, generally,
              unfeasible to solve. We then introduce a variant, in which it
              suffices to check whether a subset determines the function value
              with probability at least $\delta$ or at most $\delta$ − $\gamma$
              for 0 0, by a polynomial time algorithm unless P = NP. This holds
              even with the promise of a probability gap.",
  journal  = "J. Artif. Intell. Res.",
  volume   =  70,
  pages    = "351--387",
  month    =  jan,
  year     =  2021,
  url      = "https://www.jair.org/index.php/jair/article/view/12359",
  issn     = "1076-9757",
  doi      = "10.1613/jair.1.12359"
}

@ARTICLE{E2020-wz,
  title    = "Towards a Mathematical Understanding of Neural {Network-Based}
              Machine Learning: what we know and what we don't",
  author   = "E, Weinan and Ma, Chao and Wojtowytsch, Stephan and Wu, Lei",
  abstract = "The purpose of this article is to review the achievements made in
              the last few years towards the understanding of the reasons
              behind the success and subtleties of neural network-based machine
              learning. In the tradition of good old applied mathematics, we
              will not only give attention to rigorous mathematical results,
              but also the insight we have gained from careful numerical
              experiments as well as the analysis of simplified models. Along
              the way, we also list the open problems which we believe to be
              the most important topics for further study. This is not a
              complete overview over this quickly moving field, but we hope to
              provide a perspective which may be helpful especially to new
              researchers in the area.",
  month    =  sep,
  year     =  2020,
  url      = "http://arxiv.org/abs/2009.10713",
  arxivid  = "2009.10713"
}

@ARTICLE{Rudin2019-ci,
  title     = "Stop explaining black box machine learning models for high
               stakes decisions and use interpretable models instead",
  author    = "Rudin, Cynthia",
  abstract  = "Black box machine learning models are currently being used for
               high-stakes decision making throughout society, causing problems
               in healthcare, criminal justice and other domains. Some people
               hope that creating methods for explaining these black box models
               will alleviate some of the problems, but trying to explain black
               box models, rather than creating models that are interpretable
               in the first place, is likely to perpetuate bad practice and can
               potentially cause great harm to society. The way forward is to
               design models that are inherently interpretable. This
               Perspective clarifies the chasm between explaining black boxes
               and using inherently interpretable models, outlines several key
               reasons why explainable black boxes should be avoided in
               high-stakes decisions, identifies challenges to interpretable
               machine learning, and provides several example applications
               where interpretable models could potentially replace black box
               models in criminal justice, healthcare and computer vision.",
  journal   = "Nature Machine Intelligence",
  publisher = "Springer US",
  volume    =  1,
  number    =  5,
  pages     = "206--215",
  year      =  2019,
  url       = "http://dx.doi.org/10.1038/s42256-019-0048-x",
  issn      = "2522-5839",
  doi       = "10.1038/s42256-019-0048-x"
}

@ARTICLE{Marcucci2019-dc,
  title     = "Theory of neuromorphic computing by waves: machine learning by
               rogue waves, dispersive shocks, and solitons",
  author    = "Marcucci, Giulia and Pierangeli, Davide and Conti, Claudio",
  abstract  = "We study artificial neural networks with nonlinear waves as a
               computing reservoir. We discuss universality and the conditions
               to learn a dataset in terms of output channels and nonlinearity.
               A feed-forward three-layer model, with an encoding input layer,
               a wave layer, and a decoding readout, behaves as a conventional
               neural network in approximating mathematical functions,
               real-world datasets, and universal Boolean gates. The rank of
               the transmission matrix has a fundamental role in assessing the
               learning abilities of the wave. For a given set of training
               points, a threshold nonlinearity for universal interpolation
               exists. When considering the nonlinear Schroedinger equation,
               the use of highly nonlinear regimes implies that solitons,
               rogue, and shock waves do have a leading role in training and
               computing. Our results may enable the realization of novel
               machine learning devices by using diverse physical systems, as
               nonlinear optics, hydrodynamics, polaritonics, and Bose-Einstein
               condensates. The application of these concepts to photonics
               opens the way to a large class of accelerators and new
               computational paradigms. In complex wave systems, as multimodal
               fibers, integrated optical circuits, random, topological
               devices, and metasurfaces, nonlinear waves can be employed to
               perform computation and solve complex combinatorial
               optimization.",
  journal   = "Phys. Rev. Lett.",
  publisher = "American Physical Society",
  volume    =  125,
  number    =  9,
  pages     = "93901",
  year      =  2019,
  url       = "http://arxiv.org/abs/1912.07044",
  keywords  = "doi:10.1103/PhysRevLett.125.093901 url:https://doi",
  issn      = "0031-9007, 1079-7114",
  arxivid   = "1912.07044",
  doi       = "10.1103/PhysRevLett.125.093901"
}

@ARTICLE{Sievert2019-rq,
  title    = "Better and faster hyperparameter optimization with Dask",
  author   = "Sievert, Scott and Augspurger, Tom and Rocklin, Matthew",
  abstract = "Nearly every machine learning model requires hyperparameters,
              parameters that the user must specify before training begins and
              influence model performance. Finding the optimal set of
              hyperparameters is often a time-and resource-consuming process. A
              recent breakthrough hyperparameter optimization algorithm,
              Hyperband finds high performing hyperparameters with minimal
              training via a principled early stopping scheme for random
              hyperpa-rameter selection [LJD + 18]. This paper will provide an
              intuitive introduction to Hyperband and explain the
              implementation in Dask, a Python library that scales Python to
              larger datasets and more computational resources. The
              implementation makes adjustments to the Hyperband algorithm to
              exploit Dask's capabilities and parallel processing. In
              experiments, the Dask implementation of Hyperband rapidly finds
              high performing hyperparameters for deep learning models.",
  journal  = "Proceedings of the 18th Python in Science Conference",
  number   = "Scipy",
  pages    = "118--125",
  year     =  2019,
  url      = "http://dx.doi.org/10.25080/majora-7ddc1dd1-011",
  doi      = "10.25080/majora-7ddc1dd1-011"
}

@BOOK{Christpher_M_Bishop2006-zs,
  title     = "Pattern Recognition and Machine Learning",
  author    = "{Christpher M. Bishop}",
  abstract  = "The dramatic growth in practical applications for machine
               learning over the last ten years has been accompanied by many
               important developments in the underlying algorithms and
               techniques. For example, Bayesian methods have grown from a
               specialist niche to become mainstream, while graphical models
               have emerged as a general framework for describing and applying
               probabilistic techniques. The practical applicability of
               Bayesian methods has been greatly enhanced by the development of
               a range of approximate inference algorithms such as variational
               Bayes and expectation propagation, while new models based on
               kernels have had a significant impact on both algorithms and
               applications. This completely new textbook reflects these recent
               developments while providing a comprehensive introduction to the
               fields of pattern recognition and machine learning. It is aimed
               at advanced undergraduates or first-year PhD students, as well
               as researchers and practitioners. No previous knowledge of
               pattern recognition or machine learning concepts is assumed.
               Familiarity with multivariate calculus and basic linear algebra
               is required, and some experience in the use of probabilities
               would be helpful though not essential as the book includes a
               self-contained introduction to basic probability theory. The
               book is suitable for courses on machine learning, statistics,
               computer science, signal processing, computer vision, data
               mining, and bioinformatics. Extensive support is provided for
               course instructors, including more than 400 exercises, graded
               according to difficulty. Example solutions for a subset of the
               exercises are available from the book web site, while solutions
               for the remainder can be obtained by instructors from the
               publisher. The book is supported by a great deal of additional
               material, and the reader is encouraged to visit the book web
               site for the latest information. Christopher M. Bishop is Deputy
               Director of Microsoft Research Cambridge, and holds a Chair in
               Computer Science at the University of Edinburgh. He is a Fellow
               of Darwin College Cambridge, a Fellow of the Royal Academy of
               Engineering, and a Fellow of the Royal Society of Edinburgh. His
               previous textbook ``Neural Networks for Pattern Recognition''
               has been widely adopted.",
  publisher = "Springer-Verlag New York",
  year      =  2006,
  isbn      = "9780387310732"
}

@ARTICLE{Lecun2015-uj,
  title    = "Deep learning",
  author   = "Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
  abstract = "Deep learning allows computational models that are composed of
              multiple processing layers to learn representations of data with
              multiple levels of abstraction. These methods have dramatically
              improved the state-of-the-art in speech recognition, visual
              object recognition, object detection and many other domains such
              as drug discovery and genomics. Deep learning discovers intricate
              structure in large data sets by using the backpropagation
              algorithm to indicate how a machine should change its internal
              parameters that are used to compute the representation in each
              layer from the representation in the previous layer. Deep
              convolutional nets have brought about breakthroughs in processing
              images, video, speech and audio, whereas recurrent nets have
              shone light on sequential data such as text and speech.",
  journal  = "Nature",
  volume   =  521,
  number   =  7553,
  pages    = "436--444",
  year     =  2015,
  url      = "http://dx.doi.org/10.1038/nature14539",
  issn     = "0028-0836, 1476-4687",
  pmid     = "26017442",
  arxivid  = "1807.07987",
  doi      = "10.1038/nature14539"
}

@ARTICLE{Learning2017-fm,
  title    = "Machine learning",
  author   = "Learning, Machine",
  abstract = "Mitchell covers the field of machine learning, the study of
              algorithms that allow computer programs to automatically improve
              through experience and that automatically infer general laws from
              specific data. 1. Introduction -- 2. Concept Learning and the
              General-to-Specific Ordering -- 3. Decision Tree Learning -- 4.
              Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6.
              Bayesian Learning -- 7. Computational Learning Theory -- 8.
              Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning
              Sets of Rules -- 11. Analytical Learning -- 12. Combining
              Inductive and Analytical Learning -- 13. Reinforcement Learning.",
  journal  = "Mach. Learn.",
  volume   =  45,
  number   =  13,
  pages    = "40--48",
  year     =  2017,
  url      = "https://books.google.ca/books?id=EoYBngEACAAJ&dq=mitchell+machine+learning+1997&hl=en&sa=X&ved=0ahUKEwiomdqfj8TkAhWGslkKHRCbAtoQ6AEIKjAA",
  issn     = "0885-6125"
}

@ARTICLE{Domingos2020-hj,
  title    = "Every Model Learned by Gradient Descent Is Approximately a Kernel
              Machine",
  author   = "Domingos, Pedro",
  abstract = "Deep learning's successes are often attributed to its ability to
              automatically discover new representations of the data, rather
              than relying on handcrafted features like other learning methods.
              We show, however, that deep networks learned by the standard
              gradient descent algorithm are in fact mathematically
              approximately equivalent to kernel machines, a learning method
              that simply memorizes the data and uses it directly for
              prediction via a similarity function (the kernel). This greatly
              enhances the interpretability of deep network weights, by
              elucidating that they are effectively a superposition of the
              training examples. The network architecture incorporates
              knowledge of the target function into the kernel. This improved
              understanding should lead to better learning algorithms.",
  month    =  nov,
  year     =  2020,
  url      = "http://arxiv.org/abs/2012.00152",
  keywords = "deep learning; gradient descent; kernel machines; neural tangent
              kernel; representation learning",
  arxivid  = "2012.00152"
}

@BOOK{Vladimir_N_Vapnik2000-jw,
  title  = "The Nature of Statistical Learning Theory",
  author = "{Vladimir N. Vapnik}",
  year   =  2000,
  url    = "https://www.springer.com/gp/book/9780387987804",
  doi    = "10.1007/978-1-4757-3264-1"
}

@ARTICLE{Baye2004-nw,
  title    = "Price Dispersion in the Lab and on the Internet: Theory and
              Evidence",
  author   = "Baye, Michael R and Morgan, John",
  abstract = "Price dispersion is ubiquitous in settings that closely
              approximate textbook Bertrand competition. We show that only a
              little bounded rationality among sellers is needed to rationalize
              such dispersion. A variety of statistical tests, based on
              datasets from two independent laboratory experiments and
              structural estimates of the parameters of our models, suggest
              that bounded-rationality-based theories of price dispersion
              organize the data remarkably well. Evidence is also presented to
              suggest that the models are consistent with data from a leading
              Internet price comparison site. [ABSTRACT FROM AUTHOR]",
  journal  = "Rand J. Econ.",
  volume   =  35,
  number   =  3,
  pages    = "449",
  year     =  2004,
  url      = "http://dx.doi.org/10.2307/1593702",
  issn     = "0741-6261",
  doi      = "10.2307/1593702"
}

@BOOK{Goodfellow2016-wo,
  title     = "Deep Learning",
  author    = "Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron",
  abstract  = "Deep Learning book",
  publisher = "MIT Press",
  year      =  2016,
  url       = "http://www.deeplearningbook.org",
  doi       = "10.1016/b978-0-12-391420-0.09987-x"
}

@ARTICLE{Mackay2005-ma,
  title  = "Information Theory, Inference, and Neural Networks",
  author = "Mackay, David J C",
  pages  = "628",
  year   =  2005
}

@BOOK{Chollet2017-im,
  title  = "Deep Learning with Python",
  author = "Chollet, Fran{\c c}ois",
  year   =  2017
}

@INCOLLECTION{Iglesias2006-el,
  title     = "Neuronal cell death and synaptic pruning driven by spike-timing
               dependent plasticity",
  booktitle = "Artificial Neural Networks -- {ICANN} 2006",
  author    = "Iglesias, Javier and Villa, Alessandro E P",
  publisher = "Springer Berlin Heidelberg",
  pages     = "953--962",
  series    = "Lecture notes in computer science",
  year      =  2006,
  url       = "http://link.springer.com/10.1007/11840817_99",
  address   = "Berlin, Heidelberg",
  issn      = "0302-9743, 1611-3349",
  isbn      = "9783540386254, 9783540386278",
  doi       = "10.1007/11840817\_99"
}

@ARTICLE{Beucler2021-nn,
  title     = "Enforcing Analytic Constraints in Neural Networks Emulating
               Physical Systems",
  author    = "Beucler, Tom and Pritchard, Michael and Rasp, Stephan and Ott,
               Jordan and Baldi, Pierre and Gentine, Pierre",
  journal   = "Phys. Rev. Lett.",
  publisher = "American Physical Society",
  volume    =  126,
  number    =  9,
  pages     = "098302",
  month     =  mar,
  year      =  2021,
  url       = "https://link.aps.org/doi/10.1103/PhysRevLett.126.098302",
  issn      = "0031-9007",
  doi       = "10.1103/PhysRevLett.126.098302"
}

@ARTICLE{Lagani2021-xi,
  title         = "Hebbian {Semi-Supervised} Learning in a Sample Efficiency
                   Setting",
  author        = "Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio
                   and Amato, Giuseppe",
  abstract      = "We propose to address the issue of sample efficiency, in
                   Deep Convolutional Neural Networks (DCNN), with a
                   semisupervised training strategy that combines Hebbian
                   learning with gradient descent: all internal layers (both
                   convolutional and fully connected) are pre-trained using an
                   unsupervised approach based on Hebbian learning, and the
                   last fully connected layer (the classification layer) is
                   using Stochastic Gradient Descent (SGD). In fact, as Hebbian
                   learning is an unsupervised learning method, its potential
                   lies in the possibility of training the internal layers of a
                   DCNN without labeled examples. Only the final fully
                   connected layer has to be trained with labeled examples. We
                   performed experiments on various object recognition
                   datasets, in different regimes of sample efficiency,
                   comparing our semi-supervised (Hebbian for internal layers +
                   SGD for the final fully layer) approach with end-to-end
                   supervised backpropagation training. The results show that,
                   in regimes where the number of available labeled samples is
                   low, our semi-supervised approach outperforms full
                   backpropagation in almost all the cases.",
  month         =  mar,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.09002",
  archivePrefix = "arXiv",
  eprint        = "2103.09002",
  primaryClass  = "cs.CV",
  arxivid       = "2103.09002"
}

@ARTICLE{Murdock2021-lf,
  title         = "Reframing Neural Networks: Deep Structure in Overcomplete
                   Representations",
  author        = "Murdock, Calvin and Lucey, Simon",
  abstract      = "In comparison to classical shallow representation learning
                   techniques, deep neural networks have achieved superior
                   performance in nearly every application benchmark. But
                   despite their clear empirical advantages, it is still not
                   well understood what makes them so effective. To approach
                   this question, we introduce deep frame approximation, a
                   unifying framework for representation learning with
                   structured overcomplete frames. While exact inference
                   requires iterative optimization, it may be approximated by
                   the operations of a feed-forward deep neural network. We
                   then indirectly analyze how model capacity relates to the
                   frame structure induced by architectural hyperparameters
                   such as depth, width, and skip connections. We quantify
                   these structural differences with the deep frame potential,
                   a data-independent measure of coherence linked to
                   representation uniqueness and stability. As a criterion for
                   model selection, we show correlation with generalization
                   error on a variety of common deep network architectures such
                   as ResNets and DenseNets. We also demonstrate how recurrent
                   networks implementing iterative optimization algorithms
                   achieve performance comparable to their feed-forward
                   approximations. This connection to the established theory of
                   overcomplete representations suggests promising new
                   directions for principled deep network architecture design
                   with less reliance on ad-hoc engineering.",
  month         =  mar,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.05804",
  archivePrefix = "arXiv",
  eprint        = "2103.05804",
  primaryClass  = "cs.LG",
  arxivid       = "2103.05804"
}

@ARTICLE{Mitchell2021-jv,
  title         = "Why {AI} is Harder Than We Think",
  author        = "Mitchell, Melanie",
  abstract      = "Since its beginning in the 1950s, the field of artificial
                   intelligence has cycled several times between periods of
                   optimistic predictions and massive investment (``AI
                   spring'') and periods of disappointment, loss of confidence,
                   and reduced funding (``AI winter''). Even with today's
                   seemingly fast pace of AI breakthroughs, the development of
                   long-promised technologies such as self-driving cars,
                   housekeeping robots, and conversational companions has
                   turned out to be much harder than many people expected. One
                   reason for these repeating cycles is our limited
                   understanding of the nature and complexity of intelligence
                   itself. In this paper I describe four fallacies in common
                   assumptions made by AI researchers, which can lead to
                   overconfident predictions about the field. I conclude by
                   discussing the open questions spurred by these fallacies,
                   including the age-old challenge of imbuing machines with
                   humanlike common sense.",
  month         =  apr,
  year          =  2021,
  url           = "http://arxiv.org/abs/2104.12871",
  archivePrefix = "arXiv",
  eprint        = "2104.12871",
  primaryClass  = "cs.AI",
  arxivid       = "2104.12871"
}

@ARTICLE{Gauthier2021-up,
  title     = "Predicting hidden structure in dynamical systems",
  author    = "Gauthier, Daniel J and Fischer, Ingo",
  abstract  = "The dynamical properties of a nonlinear system can be learned
               from its time-series data, but is it possible to predict what
               happens when the system is tuned far away from its training
               values?",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  3,
  number    =  4,
  pages     = "281--282",
  month     =  apr,
  year      =  2021,
  url       = "https://www.nature.com/articles/s42256-021-00329-8",
  language  = "en",
  issn      = "2522-5839, 2522-5839",
  doi       = "10.1038/s42256-021-00329-8"
}

@BOOK{Hastie2013-tt,
  title     = "The Elements of Statistical Learning: Data Mining, Inference,
               and Prediction",
  author    = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  abstract  = "During the past decade there has been an explosion in
               computation and information technology. With it have come vast
               amounts of data in a variety of fields such as medicine,
               biology, finance, and marketing. The challenge of understanding
               these data has led to the development of new tools in the field
               of statistics, and spawned new areas such as data mining,
               machine learning, and bioinformatics. Many of these tools have
               common underpinnings but are often expressed with different
               terminology. This book describes the important ideas in these
               areas in a common conceptual framework. While the approach is
               statistical, the emphasis is on concepts rather than
               mathematics. Many examples are given, with a liberal use of
               color graphics. It is a valuable resource for statisticians and
               anyone interested in data mining in science or industry. The
               book's coverage is broad, from supervised learning (prediction)
               to unsupervised learning. The many topics include neural
               networks, support vector machines, classification trees and
               boosting---the first comprehensive treatment of this topic in
               any book. This major new edition features many topics not
               covered in the original, including graphical models, random
               forests, ensemble methods, least angle regression \& path
               algorithms for the lasso, non-negative matrix factorization, and
               spectral clustering. There is also a chapter on methods for
               ``wide'' data (p bigger than n), including multiple testing and
               false discovery rates. Trevor Hastie, Robert Tibshirani, and
               Jerome Friedman are professors of statistics at Stanford
               University. They are prominent researchers in this area: Hastie
               and Tibshirani developed generalized additive models and wrote a
               popular book of that title. Hastie co-developed much of the
               statistical modeling software and environment in R/S-PLUS and
               invented principal curves and surfaces. Tibshirani proposed the
               lasso and is co-author of the very successful An Introduction to
               the Bootstrap. Friedman is the co-inventor of many data-mining
               tools including CART, MARS, projection pursuit and gradient
               boosting.",
  publisher = "Springer Science \& Business Media",
  volume    =  99,
  pages     = "567--567",
  month     =  nov,
  year      =  2013,
  url       = "https://play.google.com/store/books/details?id=yPfZBwAAQBAJ",
  language  = "en",
  isbn      = "9780387216065"
}

@ARTICLE{Bensoussan_undated-sb,
  title   = "{MACHINE} {LEARNING} {AND} {CONTROL} {THEORY}",
  author  = "Bensoussan, Alain and Li, Yiqun and Phan, Dinh and Nguyen, Cao and
             Tran, Minh-Binh and Chi, Sheung and Yam, Phillip and Zhou, Xiang",
  url     = "http://arxiv.org/abs/2006.05604v1",
  arxivid = "2006.05604v1"
}

@ARTICLE{Gorban2012-eh,
  title         = "General H-theorem and entropies that violate the second law",
  author        = "Gorban, Alexander N",
  abstract      = "$H$-theorem states that the entropy production is
                   nonnegative and, therefore, the entropy of a closed system
                   should monotonically change in time. In information
                   processing, the entropy production is positive for random
                   transformation of signals (the information processing
                   lemma). Originally, the $H$-theorem and the information
                   processing lemma were proved for the classical
                   Boltzmann-Gibbs-Shannon entropy and for the correspondent
                   divergence (the relative entropy). Many new entropies and
                   divergences have been proposed during last decades and for
                   all of them the $H$-theorem is needed. This note proposes a
                   simple and general criterion to check whether the
                   $H$-theorem is valid for a convex divergence $H$ and
                   demonstrates that some of the popular divergences obey no
                   $H$-theorem. We consider systems with $n$ states $A_i$ that
                   obey first order kinetics (master equation). A convex
                   function $H$ is a Lyapunov function for all master equations
                   with given equilibrium if and only if its conditional minima
                   properly describe the equilibria of pair transitions $A_i
                   \rightleftharpoons A_j$. This theorem does not depend on the
                   principle of detailed balance and is valid for general
                   Markov kinetics. Elementary analysis of pair equilibria
                   demonstrates that the popular Bregman divergences like
                   Euclidean distance or Itakura-Saito distance in the space of
                   distribution cannot be the universal Lyapunov functions for
                   the first-order kinetics and can increase in Markov
                   processes. Therefore, they violate the second law and the
                   information processing lemma. In particular, for these
                   measures of information (divergences) random manipulation
                   with data may add information to data. The main results are
                   extended to nonlinear generalized mass action law kinetic
                   equations. In Appendix, a new family of the universal
                   Lyapunov functions for the generalized mass action law
                   kinetics is described.",
  month         =  dec,
  year          =  2012,
  url           = "http://arxiv.org/abs/1212.6767",
  archivePrefix = "arXiv",
  eprint        = "1212.6767",
  primaryClass  = "cond-mat.stat-mech",
  arxivid       = "1212.6767"
}

@ARTICLE{Zhang2021-rt,
  title         = "Problem Learning: Towards the Free Will of Machines",
  author        = "Zhang, Yongfeng",
  abstract      = "A machine intelligence pipeline usually consists of six
                   components: problem, representation, model, loss, optimizer
                   and metric. Researchers have worked hard trying to automate
                   many components of the pipeline. However, one key component
                   of the pipeline--problem definition--is still left mostly
                   unexplored in terms of automation. Usually, it requires
                   extensive efforts from domain experts to identify, define
                   and formulate important problems in an area. However,
                   automatically discovering research or application problems
                   for an area is beneficial since it helps to identify valid
                   and potentially important problems hidden in data that are
                   unknown to domain experts, expand the scope of tasks that we
                   can do in an area, and even inspire completely new findings.
                   This paper describes Problem Learning, which aims at
                   learning to discover and define valid and ethical problems
                   from data or from the machine's interaction with the
                   environment. We formalize problem learning as the
                   identification of valid and ethical problems in a problem
                   space and introduce several possible approaches to problem
                   learning. In a broader sense, problem learning is an
                   approach towards the free will of intelligent machines.
                   Currently, machines are still limited to solving the
                   problems defined by humans, without the ability or
                   flexibility to freely explore various possible problems that
                   are even unknown to humans. Though many machine learning
                   techniques have been developed and integrated into
                   intelligent systems, they still focus on the means rather
                   than the purpose in that machines are still solving human
                   defined problems. However, proposing good problems is
                   sometimes even more important than solving problems, because
                   a good problem can help to inspire new ideas and gain deeper
                   understandings. The paper also discusses the ethical
                   implications of problem learning under the background of
                   Responsible AI.",
  month         =  sep,
  year          =  2021,
  url           = "http://arxiv.org/abs/2109.00177",
  archivePrefix = "arXiv",
  eprint        = "2109.00177",
  primaryClass  = "cs.AI",
  arxivid       = "2109.00177"
}

@BOOK{MacKay2003-km,
  title     = "Information Theory, Inference and Learning Algorithms",
  author    = "MacKay, David J C",
  abstract  = "Information theory and inference, taught together in this
               exciting textbook, lie at the heart of many important areas of
               modern technology - communication, signal processing, data
               mining, machine learning, pattern recognition, computational
               neuroscience, bioinformatics and cryptography. The book
               introduces theory in tandem with applications. Information
               theory is taught alongside practical communication systems such
               as arithmetic coding for data compression and sparse-graph codes
               for error-correction. Inference techniques, including
               message-passing algorithms, Monte Carlo methods and variational
               approximations, are developed alongside applications to
               clustering, convolutional codes, independent component analysis,
               and neural networks. Uniquely, the book covers state-of-the-art
               error-correcting codes, including low-density-parity-check
               codes, turbo codes, and digital fountain codes - the
               twenty-first-century standards for satellite communications,
               disk drives, and data broadcast. Richly illustrated, filled with
               worked examples and over 400 exercises, some with detailed
               solutions, the book is ideal for self-learning, and for
               undergraduate or graduate courses. It also provides an
               unparalleled entry point for professionals in areas as diverse
               as computational biology, financial engineering and machine
               learning.",
  publisher = "Cambridge University Press",
  month     =  sep,
  year      =  2003,
  url       = "https://www.cambridge.org/9780521642989",
  isbn      = "9780521642989"
}

@ARTICLE{noauthor_2021-ew,
  title     = "The big question",
  abstract  = "Very large neural network models such as GPT-3, which have many
               billions of parameters, are on the rise, but so far only big
               tech has the resources to train, deploy and study such models.
               This needs to change, say Stanford AI researchers, who call for
               an investment in academic collaborations to build and study
               large neural networks.",
  journal   = "Nat Mach Intell",
  publisher = "Springer Science and Business Media LLC",
  volume    =  3,
  number    =  9,
  pages     = "737--737",
  month     =  sep,
  year      =  2021,
  url       = "https://www.nature.com/articles/s42256-021-00395-y",
  language  = "en",
  issn      = "2522-5839, 2522-5839",
  doi       = "10.1038/s42256-021-00395-y"
}

@ARTICLE{Bommasani2021-pr,
  title         = "On the opportunities and risks of foundation models",
  author        = "Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and
                   Altman, Russ and Arora, Simran and von Arx, Sydney and
                   Bernstein, Michael S and Bohg, Jeannette and Bosselut,
                   Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch,
                   Shyamal and Card, Dallas and Castellon, Rodrigo and
                   Chatterji, Niladri and Chen, Annie and Creel, Kathleen and
                   Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and
                   Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and
                   Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and
                   Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and
                   Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha,
                   Neel and Hashimoto, Tatsunori and Henderson, Peter and
                   Hewitt, John and Ho, Daniel E and Hong, Jenny and Hsu, Kyle
                   and Huang, Jing and Icard, Thomas and Jain, Saahil and
                   Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti,
                   Siddharth and Keeling, Geoff and Khani, Fereshte and
                   Khattab, Omar and Kohd, Pang Wei and Krass, Mark and
                   Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and
                   Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec,
                   Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen
                   and Ma, Tengyu and Malik, Ali and Manning, Christopher D and
                   Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele
                   and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak
                   and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and
                   Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and
                   Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung
                   and Piech, Chris and Portelance, Eva and Potts, Christopher
                   and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and
                   Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan,
                   Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa,
                   Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan,
                   Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin
                   W and Tram{\`e}r, Florian and Wang, Rose E and Wang, William
                   and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang
                   Michael and Yasunaga, Michihiro and You, Jiaxuan and
                   Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and
                   Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou,
                   Kaitlyn and Liang, Percy",
  abstract      = "AI is undergoing a paradigm shift with the rise of models
                   (e.g., BERT, DALL-E, GPT-3) that are trained on broad data
                   at scale and are adaptable to a wide range of downstream
                   tasks. We call these models foundation models to underscore
                   their critically central yet incomplete character. This
                   report provides a thorough account of the opportunities and
                   risks of foundation models, ranging from their capabilities
                   (e.g., language, vision, robotics, reasoning, human
                   interaction) and technical principles(e.g., model
                   architectures, training procedures, data, systems, security,
                   evaluation, theory) to their applications (e.g., law,
                   healthcare, education) and societal impact (e.g., inequity,
                   misuse, economic and environmental impact, legal and ethical
                   considerations). Though foundation models are based on
                   standard deep learning and transfer learning, their scale
                   results in new emergent capabilities,and their effectiveness
                   across so many tasks incentivizes homogenization.
                   Homogenization provides powerful leverage but demands
                   caution, as the defects of the foundation model are
                   inherited by all the adapted models downstream. Despite the
                   impending widespread deployment of foundation models, we
                   currently lack a clear understanding of how they work, when
                   they fail, and what they are even capable of due to their
                   emergent properties. To tackle these questions, we believe
                   much of the critical research on foundation models will
                   require deep interdisciplinary collaboration commensurate
                   with their fundamentally sociotechnical nature.",
  month         =  aug,
  year          =  2021,
  url           = "http://arxiv.org/abs/2108.07258",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2108.07258",
  primaryClass  = "cs.LG",
  arxivid       = "2108.07258"
}
